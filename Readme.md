This project report explores the Naïve - Bayes and Logistic regression classifiers, discusses the implementation of both the classifiers
on the problem of classifying the MNIST dataset that contains the images of handwritten numbers. Assumption on probability of each class 
are made to classify the images.

Conclusions:
1. Logistic regression is more efficient than the Naïve – Bayes algorithm, the overall accuracy obtained is 91.85% where in Naïve – Bayes
ended up 61.82% test accuracy. Logistic Regression being complex as compared to the Naïve – Bayes algorithm, Logistic Regression model 
performed well in classifying the images.
2. Logistic Regression is more computationally expensive than Naïve Bayes. The Naïve Bayes took less than a minute to train and predict 
the labels, whereas Logistic Regression took about an hour to train and predict the labels.
3. Naïve Bayes classifier assumes all the dimensions as independent to one another which is not true.

References
[1] CS5841 - Machine learning by Dr. Timoty Havens, Lecture slides.
[2] Lazy Programmers- Source of deep learning and Machine Learning.
[3] K. Elissa, "Title of paper if known," unpublished.
[4] Logistic Regression Overview Towards Data Science..
[5] Pattern recognition and Machine Learning by Bishop.
[6] DataCamp Ridge and Lasso Regularization.
[7] Machine Learning Mastery - Naive Bayes.
[8] Kaggle Logistic Regression without Scikit Learn.

